# Flutter AI Agent SDK

A high-performance, extensible AI Agent SDK for Flutter with voice interaction, LLM integration, and tool execution capabilities.

## âœ¨ Features

- ğŸ™ï¸ **Voice Interaction**: Built-in STT/TTS with native platform support
- ğŸ¤– **Multiple LLM Providers**: OpenAI, Anthropic, and custom providers
- ğŸ› ï¸ **Tool/Function Calling**: Execute custom functions from AI responses
- ğŸ’¾ **Conversation Memory**: Short-term and long-term memory management
- ğŸ”„ **Streaming Support**: Real-time response streaming
- ğŸ¯ **Turn Detection**: VAD, push-to-talk, and hybrid modes
- ğŸ“¦ **Pure Dart**: No platform-specific code required
- âš¡ **High Performance**: Optimized for mobile devices

## ğŸš€ Installation

Add to your `pubspec.yaml`:

```yaml
dependencies:
  flutter_ai_agent_sdk:
    path: ../flutter_ai_agent_sdk
```

## ğŸ“– Quick Start

### 1. Create an LLM Provider

```dart
import 'package:flutter_ai_agent_sdk/flutter_ai_agent_sdk.dart';

final llmProvider = OpenAIProvider(
  apiKey: 'your-api-key',
  model: 'gpt-4-turbo-preview',
);
```

### 2. Configure Your Agent

```dart
final config = AgentConfig(
  name: 'My Assistant',
  instructions: 'You are a helpful AI assistant.',
  llmProvider: llmProvider,
  sttService: NativeSTTService(),
  ttsService: NativeTTSService(),
  turnDetection: TurnDetectionConfig(
    mode: TurnDetectionMode.vad,
    silenceThreshold: Duration(milliseconds: 700),
  ),
  tools: [
    // Add your custom tools here
  ],
);
```

### 3. Create and Use Agent

```dart
final agent = VoiceAgent(config: config);
final session = await agent.createSession();

// Send text message
await session.sendMessage('Hello!');

// Start voice interaction
await session.startListening();

// Listen to events
session.events.listen((event) {
  if (event is MessageReceivedEvent) {
    print('Assistant: ${event.message.content}');
  }
});

// Listen to state changes
session.state.listen((status) {
  print('State: ${status.state}');
});
```

## ğŸ› ï¸ Creating Custom Tools

```dart
final weatherTool = FunctionTool(
  name: 'get_weather',
  description: 'Get current weather for a location',
  parameters: {
    'type': 'object',
    'properties': {
      'location': {'type': 'string', 'description': 'City name'},
      'unit': {'type': 'string', 'enum': ['celsius', 'fahrenheit']},
    },
    'required': ['location'],
  },
  function: (args) async {
    final location = args['location'];
    final unit = args['unit'] ?? 'celsius';
    
    // Your weather API call here
    return {
      'temperature': 22,
      'condition': 'sunny',
      'location': location,
      'unit': unit,
    };
  },
);
```

## ğŸ—ï¸ Architecture

```
flutter_ai_agent_sdk/
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ core/           # Core agent logic
â”‚   â”‚   â”‚   â”œâ”€â”€ agents/     # VoiceAgent, config
â”‚   â”‚   â”‚   â”œâ”€â”€ sessions/   # Session management
â”‚   â”‚   â”‚   â”œâ”€â”€ events/     # Event system
â”‚   â”‚   â”‚   â””â”€â”€ models/     # Data models
â”‚   â”‚   â”œâ”€â”€ voice/          # Voice processing
â”‚   â”‚   â”‚   â”œâ”€â”€ stt/        # Speech-to-text
â”‚   â”‚   â”‚   â”œâ”€â”€ tts/        # Text-to-speech
â”‚   â”‚   â”‚   â”œâ”€â”€ vad/        # Voice activity detection
â”‚   â”‚   â”‚   â””â”€â”€ audio/      # Audio utilities
â”‚   â”‚   â”œâ”€â”€ llm/            # LLM integration
â”‚   â”‚   â”‚   â”œâ”€â”€ providers/  # Provider implementations
â”‚   â”‚   â”‚   â”œâ”€â”€ chat/       # Chat context
â”‚   â”‚   â”‚   â””â”€â”€ streaming/  # Stream processing
â”‚   â”‚   â”œâ”€â”€ tools/          # Tool execution
â”‚   â”‚   â”œâ”€â”€ memory/         # Memory management
â”‚   â”‚   â””â”€â”€ utils/          # Utilities
â”‚   â””â”€â”€ flutter_ai_agent_sdk.dart
â”œâ”€â”€ example/                # Example app
â””â”€â”€ test/                   # Tests
```

## ğŸ”Œ Supported LLM Providers

### OpenAI
```dart
OpenAIProvider(
  apiKey: 'sk-...',
  model: 'gpt-4-turbo-preview',
)
```

### Anthropic
```dart
AnthropicProvider(
  apiKey: 'sk-ant-...',
  model: 'claude-3-sonnet-20240229',
)
```

### Custom Provider
```dart
class MyCustomProvider extends LLMProvider {
  @override
  String get name => 'MyProvider';
  
  @override
  Future<LLMResponse> generate({
    required List<Message> messages,
    List<Tool>? tools,
    Map<String, dynamic>? parameters,
  }) async {
    // Your implementation
  }
  
  @override
  Stream<LLMResponse> generateStream({
    required List<Message> messages,
    List<Tool>? tools,
    Map<String, dynamic>? parameters,
  }) async* {
    // Your streaming implementation
  }
}
```

## ğŸ¯ Turn Detection Modes

- **VAD** (Voice Activity Detection): Automatic speech detection
- **Push-to-Talk**: Manual button control
- **Server VAD**: Server-side detection (e.g., OpenAI Realtime)
- **Hybrid**: Combined VAD + silence detection

## ğŸ“± Platform Support

- âœ… iOS
- âœ… Android
- âœ… Web (limited voice features)
- âœ… macOS
- âœ… Windows
- âœ… Linux

## ğŸ§ª Testing

```bash
flutter test
```

## ğŸ“„ License

MIT License

## ğŸ¤ Contributing

Contributions welcome! Please read CONTRIBUTING.md first.

## ğŸ“ Support

- Documentation: [link]
- Issues: [GitHub Issues]
- Discord: [link]
